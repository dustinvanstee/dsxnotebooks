{"metadata": {"language_info": {"name": "scala"}, "kernelspec": {"display_name": "Scala 2.10", "language": "scala", "name": "spark"}}, "nbformat": 4, "cells": [{"metadata": {}, "cell_type": "markdown", "source": "#Data Wrangling"}, {"metadata": {}, "cell_type": "markdown", "source": "###Print Spark Version and Import Libraries"}, {"outputs": [{"output_type": "stream", "text": "Spark Version : 1.6.0\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 68, "source": "println(\"Spark Version : \" + sc.version)\n\nimport org.apache.commons.io.IOUtils\nimport java.net.URL\nimport java.nio.charset.Charset\nimport com.databricks.spark.csv\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType,DoubleType,DateType,TimestampType};\nimport scala.util.matching.Regex\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types.{StructType,StructField,StringType,DoubleType};\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions.{udf,col,concat,lit,when,not,avg,asc}\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Add Boilerplate for loading data from Swift object store - REVAMP"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 31, "source": "def setConfig(credentials : scala.collection.mutable.HashMap[String, String]) = {\n    val prefix = \"fs.swift.service.\" + credentials(\"name\") \n    var hconf = sc.getConf\n    hconf.set(prefix + \".auth.url\", credentials(\"auth_url\")+\"/v3/auth/tokens\")\n    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n    hconf.set(prefix + \".tenant\", credentials(\"project_id\"))\n    hconf.set(prefix + \".username\", credentials(\"user_id\"))\n    hconf.set(prefix + \".password\", credentials(\"password\"))\n    hconf.set(prefix + \".http.port\", \"8080\")\n    hconf.set(prefix + \".region\", credentials(\"region\"))\n    hconf.set(prefix + \".public\", \"True\")   \n}\n\nvar credentials_1 = scala.collection.mutable.HashMap[String, String](\n  \"auth_url\"->\"https://identity.open.softlayer.com\",\n  \"project\"->\"object_storage_0dc58f04_2c63_4761_9aed_ef4449bae858\",\n  \"project_id\"->\"f4162fef734849528c38937ca26ba3a7\",\n  \"region\"->\"dallas\",\n  \"user_id\"->\"e4fc729439f044199998e68ef469e3c5\",\n  \"domain_id\"->\"7527e49e459a4de4a41373a7e8fb444f\",\n  \"domain_name\"->\"984499\",\n  \"username\"->\"Admin_66474152b3dcb39af2943f3f79bb2618f028659f\",\n  \"password\"->\"\"\"pIv/Y^MOZ17^LQ{j\"\"\",\n  \"filename\"->\"scores_nba.test.dat\",\n  \"container\"->\"notebooks\",\n  \"tenantId\"->\"s69c-1c345cae431193-591ae43264f0\"\n)\n\ncredentials_1(\"name\") = \"spark\"\nsetConfig(credentials_1)\n//val nbafile = sc.textFile(\"swift://notebooks.\" + credentials_1(\"name\") + \"/\" + credentials_1(\"filename\"))\n\nval nbafile  = \"swift://notebooks.spark/scores_nba.test.dat\"\nval oddsfile = \"swift://notebooks.spark/nbaodds_042516.xml\"\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Use local filesystem to store small initial files from GitHub"}, {"outputs": [{"output_type": "stream", "text": "--2016-09-14 09:00:31--  https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/nbaodds_042516.xml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21937 (21K) [text/plain]\nSaving to: 'nbaodds_042516.xml.2'\n\n     0K .......... .......... .                               100% 22.6M=0.001s\n\n2016-09-14 09:00:31 (22.6 MB/s) - 'nbaodds_042516.xml.2' saved [21937/21937]\n\n--2016-09-14 09:00:33--  https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/scores_nba.test.dat\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1301197 (1.2M) [text/plain]\nSaving to: 'scores_nba.test.dat.1'\n\n     0K .......... .......... .......... .......... ..........  3% 24.1M 0s\n    50K .......... .......... .......... .......... ..........  7% 25.1M 0s\n   100K .......... .......... .......... .......... .......... 11% 13.3M 0s\n   150K .......... .......... .......... .......... .......... 15% 29.8M 0s\n   200K .......... .......... .......... .......... .......... 19% 20.1M 0s\n   250K .......... .......... .......... .......... .......... 23% 5.88M 0s\n   300K .......... .......... .......... .......... .......... 27% 39.3M 0s\n   350K .......... .......... .......... .......... .......... 31% 21.8M 0s\n   400K .......... .......... .......... .......... .......... 35% 36.7M 0s\n   450K .......... .......... .......... .......... .......... 39% 24.8M 0s\n   500K .......... .......... .......... .......... .......... 43% 32.2M 0s\n   550K .......... .......... .......... .......... .......... 47% 25.5M 0s\n   600K .......... .......... .......... .......... .......... 51% 31.7M 0s\n   650K .......... .......... .......... .......... .......... 55% 27.8M 0s\n   700K .......... .......... .......... .......... .......... 59% 29.0M 0s\n   750K .......... .......... .......... .......... .......... 62% 22.0M 0s\n   800K .......... .......... .......... .......... .......... 66% 29.8M 0s\n   850K .......... .......... .......... .......... .......... 70% 23.5M 0s\n   900K .......... .......... .......... .......... .......... 74% 38.5M 0s\n   950K .......... .......... .......... .......... .......... 78% 27.4M 0s\n  1000K .......... .......... .......... .......... .......... 82% 31.1M 0s\n  1050K .......... .......... .......... .......... .......... 86% 41.7M 0s\n  1100K .......... .......... .......... .......... .......... 90% 30.1M 0s\n  1150K .......... .......... .......... .......... .......... 94% 39.4M 0s\n  1200K .......... .......... .......... .......... .......... 98% 34.2M 0s\n  1250K .......... ..........                                 100%  143M=0.05s\n\n2016-09-14 09:00:33 (24.2 MB/s) - 'scores_nba.test.dat.1' saved [1301197/1301197]\n\ntotal 26227464\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1957260 Apr 25 12:55 Snap.20160425.125524.9652.0006.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  105768682 Apr 25 12:55 heapdump.20160425.125524.9652.0001.phd\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1331900416 Apr 25 12:55 core.20160425.125524.9652.0002.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  105768682 Apr 25 12:55 heapdump.20160425.125524.9652.0003.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3331367 Apr 25 12:55 javacore.20160425.125524.9652.0004.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3330652 Apr 25 12:55 javacore.20160425.125524.9652.0005.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  105768682 Apr 25 12:55 heapdump.20160425.125524.9652.0007.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  105768682 Apr 25 12:55 heapdump.20160425.125524.9652.0008.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3328332 Apr 25 12:55 javacore.20160425.125524.9652.0009.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3327754 Apr 25 12:55 javacore.20160425.125524.9652.0010.txt\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1249665024 Apr 25 13:07 core.20160425.130726.28716.0001.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   91472855 Apr 25 13:07 heapdump.20160425.130726.28716.0002.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   91472855 Apr 25 13:07 heapdump.20160425.130726.28716.0003.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   91472855 Apr 25 13:07 heapdump.20160425.130726.28716.0004.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   91472855 Apr 25 13:07 heapdump.20160425.130726.28716.0005.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2030988 Apr 25 13:07 Snap.20160425.130726.28716.0010.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2704973 Apr 25 13:08 javacore.20160425.130726.28716.0006.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2704983 Apr 25 13:08 javacore.20160425.130726.28716.0007.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2704255 Apr 25 13:08 javacore.20160425.130726.28716.0008.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2703594 Apr 25 13:08 javacore.20160425.130726.28716.0009.txt\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.39989\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.39994\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.39997\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40004\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40013\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40020\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40032\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40038\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40049\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users     253952 Apr 30 21:40 core.40058\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10408 May 10 14:02 index.html\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10399 May 10 14:05 index.html.1\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10455 May 10 14:05 index.html.2\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10431 May 10 14:08 index.html.3\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10403 May 10 14:08 index.html.4\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10408 May 10 14:08 index.html.5\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10464 May 10 14:09 index.html.6\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10385 May 10 14:09 index.html.7\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10400 May 10 14:10 index.html.8\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10401 May 10 14:10 index.html.9\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10431 May 10 14:10 index.html.10\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10456 May 10 14:12 index.html.11\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10463 May 10 14:15 index.html.12\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10442 May 10 14:18 index.html.13\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10440 May 10 14:19 index.html.14\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10499 May 10 14:22 index.html.15\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10432 May 10 14:23 index.html.16\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10456 May 10 14:26 index.html.17\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      10437 May 10 14:43 index.html.18\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  114334441 May 10 15:42 heapdump.20160510.154203.26233.0001.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  114334441 May 10 15:42 heapdump.20160510.154203.26233.0002.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  114334441 May 10 15:42 heapdump.20160510.154203.26233.0003.phd\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1250758656 May 10 15:42 core.20160510.154203.26233.0004.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  114334441 May 10 15:42 heapdump.20160510.154203.26233.0005.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1776776 May 10 15:42 Snap.20160510.154203.26233.0010.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    5413014 May 10 15:42 javacore.20160510.154203.26233.0006.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    5412559 May 10 15:42 javacore.20160510.154203.26233.0007.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    5412050 May 10 15:42 javacore.20160510.154203.26233.0008.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    5411334 May 10 15:42 javacore.20160510.154203.26233.0009.txt\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1235316736 May 13 09:44 core.20160513.094358.26220.0001.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   36803003 May 13 09:44 heapdump.20160513.094358.26220.0002.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    2050440 May 13 09:44 javacore.20160513.094358.26220.0003.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1933064 May 13 09:44 Snap.20160513.094358.26220.0004.trc\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1212133376 May 13 09:56 core.20160513.095648.30427.0001.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   50696747 May 13 09:56 heapdump.20160513.095648.30427.0002.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1939112 May 13 09:56 javacore.20160513.095648.30427.0003.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1785608 May 13 09:56 Snap.20160513.095648.30427.0004.trc\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1349029888 May 18 12:48 core.20160518.124833.22387.0001.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  182683347 May 18 12:48 heapdump.20160518.124833.22387.0002.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  182683347 May 18 12:48 heapdump.20160518.124833.22387.0003.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    8150410 May 18 12:48 javacore.20160518.124833.22387.0004.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1859336 May 18 12:48 Snap.20160518.124833.22387.0006.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    7915692 May 18 12:48 javacore.20160518.124833.22387.0005.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      81672 May 18 12:48 Snap.20160518.124833.22387.0007.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  182711052 May 18 12:48 heapdump.20160518.124845.22387.0008.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    7909633 May 18 12:48 javacore.20160518.124845.22387.0009.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      57096 May 18 12:48 Snap.20160518.124845.22387.0010.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users  182756273 May 18 12:48 heapdump.20160518.124851.22387.0011.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    7913746 May 18 12:48 javacore.20160518.124851.22387.0012.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users     114440 May 18 12:48 Snap.20160518.124851.22387.0013.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    7239680 Jun  1 00:00 systemml-0.10.0-incubating.tar\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   47853436 Jun 17 10:36 amazon0601.txt\ndrwxr-xr-x 3 s69c-1c345cae431193-591ae43264f0 users       4096 Jun 17 10:43 systemml-0.10.0-incubating\ndrwxrwxrwx 2 s69c-1c345cae431193-591ae43264f0 users       4096 Jun 17 12:57 scratch_space\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   90025691 Jun 18 11:06 heapdump.20160618.110600.6448.0001.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   90025691 Jun 18 11:06 heapdump.20160618.110600.6448.0002.phd\n-rw------- 1 s69c-1c345cae431193-591ae43264f0 users 1260810240 Jun 18 11:06 core.20160618.110600.6448.0003.dmp\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3367040 Jun 18 11:06 javacore.20160618.110600.6448.0004.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1697792 Jun 18 11:06 Snap.20160618.110600.6448.0007.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3318848 Jun 18 11:06 javacore.20160618.110600.6448.0005.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   90025691 Jun 18 11:06 heapdump.20160618.110600.6448.0006.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3317855 Jun 18 11:06 javacore.20160618.110600.6448.0009.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      59392 Jun 18 11:06 Snap.20160618.110600.6448.0010.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   90025793 Jun 18 11:06 heapdump.20160618.110618.6448.0011.phd\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    3351703 Jun 18 11:06 javacore.20160618.110618.6448.0012.txt\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users     567296 Jun 18 11:06 Snap.20160618.110618.6448.0013.trc\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users     446287 Jul 19 11:08 world_bank.json.gz\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      14253 Jul 19 11:08 SIGHTINGS.csv\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    7483128 Jul 19 14:18 OnlineRetail.csv.gz\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users       1697 Jul 28 14:22 mtcars.csv\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users   11807430 Aug  9 10:53 amazon0601.txt.gz\ndrwxr-xr-x 5 s69c-1c345cae431193-591ae43264f0 users       4096 Aug  9 10:54 metastore_db\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users       1174 Aug  9 10:54 derby.log\ndrwxr-xr-x 5 s69c-1c345cae431193-591ae43264f0 users       4096 Sep 12 15:22 ..\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      24903 Sep 13 03:38 TrapezoidRule.ipynb\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      15212 Sep 13 03:38 WeatherPrecipitation.ipynb\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users          0 Sep 13 12:15 junk\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      21937 Sep 13 12:16 nbaodds_042516.xml\ndrwxr-xr-x 2 s69c-1c345cae431193-591ae43264f0 users      16384 Sep 13 12:39 nba-datawrangle-lrDF.csv\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      21937 Sep 14 08:48 nbaodds_042516.xml.1\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1301197 Sep 14 08:48 scores_nba.test.dat\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users      21937 Sep 14 09:00 nbaodds_042516.xml.2\ndrwxr-xr-x 6 s69c-1c345cae431193-591ae43264f0 users       8192 Sep 14 09:00 .\n-rw-r--r-- 1 s69c-1c345cae431193-591ae43264f0 users    1301197 Sep 14 09:00 scores_nba.test.dat.1\nbasketball_nba.040516.xml:        <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\nbasketball_nba.040516.xml:        <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\nbasketball_nba.040516.xml:        <title>Charlotte 4.0 O (200.5) 155.0 | Toronto -4.0 U (200.5) -175.0 (Apr 05, 2016 07:40 PM)</title>\nbasketball_nba.040516.xml:        <title>Phoenix 14.5 O (207.5) -110.0 | Atlanta -14.5 U (207.5) -110.0 (Apr 05, 2016 08:10 PM)</title>\nbasketball_nba.040516.xml:        <title>Chicago -3.0 O (201.5) -150.0 | Memphis 3.0 U (201.5) 130.0 (Apr 05, 2016 08:10 PM)</title>\n", "name": "stdout"}, {"output_type": "execute_result", "metadata": {}, "execution_count": 10, "data": {"text/plain": "0"}}, {"output_type": "stream", "text": "2016-04-05,15:06:16,Phoenix,0,Atlanta,0,(8:00 PM ET),48.0,400829044\n2016-04-05,15:06:16,Chicago,0,Memphis,0,(8:00 PM ET),48.0,400829045\n2016-04-05,15:06:16,Cleveland,0,Milwaukee,0,(8:00 PM ET),48.0,400829046\n2016-04-05,15:06:16,Oklahoma City,0,Denver,0,(9:00 PM ET),48.0,400829047\n2016-04-05,15:06:16,New Orleans,0,Philadelphia,0,(7:00 PM ET),48.0,400829041\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 10, "source": "import sys.process._\n\n// Store the current working directory for this notebook in variable yourDir\nval yourDir=\"pwd\".!!.replaceAll(\"\\n\", \"\")\n\n// Get the source data files from GitHub\n\"wget https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/nbaodds_042516.xml\".!\n\"wget https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/scores_nba.test.dat\".!\n\n// Make sure the file is in the directory\n\"ls -lart\".!\n\n// Setup paths to data files for later use in notebooks\nval nbafile  = yourDir+\"/scores_nba.test.dat\"\nval oddsfile = yourDir+\"/nbaodds_042516.xml\"\n\n// Print out a few lines from the files\nSeq(\"head\", \"-n\", \"5\", oddsfile).! \nSeq(\"head\", \"-n\", \"5\", nbafile).! \n\n"}, {"outputs": [{"output_type": "stream", "text": "Filesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       930G  119G  812G  13% /\ndevtmpfs        189G     0  189G   0% /dev\ntmpfs           189G     0  189G   0% /dev/shm\ntmpfs           189G  258M  189G   1% /run\ntmpfs           189G     0  189G   0% /sys/fs/cgroup\n/dev/sde1       3.6T   89M  3.4T   1% /disk4\n/dev/sdh1       3.6T   89M  3.4T   1% /disk7\n/dev/sdg1       3.6T   89M  3.4T   1% /disk6\n/dev/sdj1       3.6T   89M  3.4T   1% /disk9\n/dev/sdb1       3.6T  9.4G  3.4T   1% /disk1\n/dev/sdi1       3.6T   89M  3.4T   1% /disk8\n/dev/sdf1       3.6T   89M  3.4T   1% /disk5\n/dev/sdd1       3.6T   89M  3.4T   1% /disk3\n/dev/sdc1       3.6T   89M  3.4T   1% /disk2\n/dev/sda1       253M  161M   93M  64% /boot\ntmpfs            38G     0   38G   0% /run/user/0\n/dev/fs01       246T   80T  167T  33% /gpfs/global_fs01\n", "name": "stdout"}, {"output_type": "execute_result", "metadata": {}, "execution_count": 1, "data": {"text/plain": "0"}}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 1, "source": "import sys.process._\n\"df -h\".!"}, {"metadata": {}, "cell_type": "markdown", "source": "### Helpers ... Lookup Tables"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 11, "source": "val teamMap = Map(\n  \"Atlanta\" -> \"atl\",\n  \"Boston\"  -> \"bos\",\n  \"Brooklyn\"  -> \"bkn\",\n  \"Charlotte\"  -> \"cha\",\n  \"Chicago\"  -> \"chi\",\n  \"Cleveland\"  -> \"cle\",\n  \"Dallas\"  -> \"dal\",\n  \"Denver\"  -> \"den\",\n  \"Detroit\"  -> \"det\",\n  \"Golden State\"  -> \"gst\",\n  \"Houston\"  -> \"hou\",\n  \"Indiana\"  -> \"ind\",\n  \"LA Clippers\"  -> \"lac\",\n  \"LA Lakers\"  -> \"lal\",\n  \"Memphis\"  -> \"mem\",\n  \"Miami\"  -> \"mia\",\n  \"Milwaukee\"  -> \"mil\",\n  \"Minnesota\"  -> \"min\",\n  \"New Orleans\"  -> \"nor\",\n  \"New York\"  -> \"nyk\",\n  \"Oklahoma City\"  -> \"okc\",\n  \"Orlando\"  -> \"orl\",\n  \"Philadelphia\"  -> \"phi\",\n  \"Phila.\"  -> \"phi\",\n  \"Phoenix\"  -> \"pho\",\n  \"Portland\"  -> \"por\",\n  \"Sacramento\" -> \"sac\",\n  \"San Antonio\"  -> \"san\",\n  \"Toronto\"  -> \"tor\",\n  \"Utah\"  -> \"uta\",\n  \"Washington\"  -> \"wsh\")\n  \nval monthMap = Map(\n    \"Jan\"-> \"01\",\n    \"Feb\"-> \"02\",\n    \"Mar\"-> \"03\",\n    \"Apr\"-> \"04\",\n    \"May\"-> \"05\",\n    \"Jun\"-> \"06\",\n    \"Jul\"-> \"07\",\n    \"Aug\"-> \"08\",\n    \"Sep\"-> \"09\",\n    \"Oct\"-> \"10\",\n    \"Nov\"-> \"11\",\n    \"Dec\"-> \"12\"\n)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Inspect Score Data ( remove ??)"}, {"outputs": [{"output_type": "stream", "text": "swift://notebooks.spark/scores_nba.test.dat\nhead: cannot open 'swift://notebooks.spark/scores_nba.test.dat' for reading: No such file or directory\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/sda3       930G  119G  812G  13% /\ndevtmpfs        189G     0  189G   0% /dev\ntmpfs           189G     0  189G   0% /dev/shm\ntmpfs           189G  226M  189G   1% /run\ntmpfs           189G     0  189G   0% /sys/fs/cgroup\n/dev/sde1       3.6T   89M  3.4T   1% /disk4\n/dev/sdh1       3.6T   89M  3.4T   1% /disk7\n/dev/sdg1       3.6T   89M  3.4T   1% /disk6\n/dev/sdj1       3.6T   89M  3.4T   1% /disk9\n/dev/sdb1       3.6T  9.4G  3.4T   1% /disk1\n/dev/sdi1       3.6T   89M  3.4T   1% /disk8\n/dev/sdf1       3.6T   89M  3.4T   1% /disk5\n/dev/sdd1       3.6T   89M  3.4T   1% /disk3\n/dev/sdc1       3.6T   89M  3.4T   1% /disk2\n/dev/sda1       253M  161M   93M  64% /boot\ntmpfs            38G     0   38G   0% /run/user/0\n/dev/fs01       246T   81T  166T  33% /gpfs/global_fs01\n/gpfs/global_fs01/sym_shared/YPProdSpark/user/s69c-1c345cae431193-591ae43264f0/notebook/notebooks\n", "name": "stdout"}, {"output_type": "execute_result", "metadata": {}, "execution_count": 33, "data": {"text/plain": "0"}}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 33, "source": "// TODO ## cat a file in swift ?? Prob not possible\nprintln(nbafile)\nimport sys.process._\n\"head swift://notebooks.spark/scores_nba.test.dat\".!\n\"df -h\".!\n\"pwd\".!\n\"touch /gpfs/global_fs01/sym_shared/YPProdSpark/user/s69c-1c345cae431193-591ae43264f0/notebook/notebooks/junk\".!"}, {"metadata": {}, "cell_type": "markdown", "source": "###Load In NBA Score Data Sets"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 12, "source": "// Since I dont have a header in the data set, I want to specify the column metadata\nval sqlContext = new SQLContext(sc)\n// Need this for my shorthand $ notation\nimport sqlContext.implicits._  \n\nval customSchema = StructType(Array(\n    StructField(\"dateOrig\", DateType, true),\n    StructField(\"ts\", StringType, true),\n    StructField(\"teamlonga\", StringType, true),\n    StructField(\"scorea\", IntegerType, true),\n    StructField(\"teamlongb\", StringType, true),\n    StructField(\"scoreb\", IntegerType, true),\n    StructField(\"time-string\", StringType, true),\n    StructField(\"timeleft\", DoubleType, true),\n    StructField(\"gameid\", IntegerType, true)\n    ))\n\n// This line reads in the file and parses it with a CSV reader\n// Use first line of all files as header.\n// Automatically infer data types)\nvar rtscoresAndFinalDF = sqlContext.read.format(\"com.databricks.spark.csv\").\n    option(\"header\", \"false\").\n    option(\"inferSchema\", \"false\").\n    option(\"nullValue\", \"empty\").\n    option(\"dateFormat\", \"yyyy-MM-dd\").\n    option(\"mode\",\"DROPMALFORMED\").\n    schema(customSchema).load(nbafile) \n    \n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Inspect Historical Score Data"}, {"outputs": [{"output_type": "stream", "text": "+----------+--------+-------------+------+------------+------+------------+--------+---------+\n|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb| time-string|timeleft|   gameid|\n+----------+--------+-------------+------+------------+------+------------+--------+---------+\n|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|\n|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|\n|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|\n|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|\n|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|\n+----------+--------+-------------+------+------------+------+------------+--------+---------+\nonly showing top 5 rows\n\n+----------+--------+-----------+------+------------+------+-----------+--------+---------+\n|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|time-string|timeleft|   gameid|\n+----------+--------+-----------+------+------------+------+-----------+--------+---------+\n|2016-04-05|21:22:09|New Orleans|    93|Philadelphia|   107|    (FINAL)|     0.0|400829041|\n|2016-04-05|22:08:42|  Charlotte|    90|     Toronto|    96|    (FINAL)|     0.0|400829043|\n|2016-04-05|22:25:25|    Chicago|    92|     Memphis|   108|    (FINAL)|     0.0|400829045|\n|2016-04-05|22:28:58|    Phoenix|    90|     Atlanta|   103|    (FINAL)|     0.0|400829044|\n|2016-04-05|22:30:29|  Cleveland|   109|   Milwaukee|    80|    (FINAL)|     0.0|400829046|\n+----------+--------+-----------+------+------------+------+-----------+--------+---------+\nonly showing top 5 rows\n\n+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n|  dateOrig|      ts|  teamlonga|scorea|   teamlongb|scoreb|  time-string|     timeleft|   gameid|\n+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\n|2016-04-05|19:23:42|New Orleans|    23|Philadelphia|    12|(4:39 IN 1ST)|        40.65|400829041|\n|2016-04-05|19:23:57|New Orleans|    23|Philadelphia|    14|(4:05 IN 1ST)|40.0833333333|400829041|\n|2016-04-05|19:24:13|New Orleans|    23|Philadelphia|    14|(3:41 IN 1ST)|39.6833333333|400829041|\n|2016-04-05|19:24:28|New Orleans|    23|Philadelphia|    14|(3:32 IN 1ST)|39.5333333333|400829041|\n|2016-04-05|19:24:43|New Orleans|    23|Philadelphia|    16|(3:24 IN 1ST)|         39.4|400829041|\n+----------+--------+-----------+------+------------+------+-------------+-------------+---------+\nonly showing top 5 rows\n\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 13, "source": "rtscoresAndFinalDF.cache()\nrtscoresAndFinalDF.show(5)\nrtscoresAndFinalDF.filter($\"time-string\".contains(\"FINAL\")).show(5)\nrtscoresAndFinalDF.filter(rtscoresAndFinalDF(\"time-string\").contains(\"1ST\")).show(5)"}, {"metadata": {}, "cell_type": "markdown", "source": "### UDFs For Creating Extra Columns In Real Time Data Frame"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 36, "source": "////////////////////////////////////////////////////////////\n// These were a couple custom UDF's I needed to cleanse the data \n// and also to add a few features based on a proprietary way of combining\n// the score with the time left.\n\n// Create new team name column.. do simple lookup conversion with a UDF\nval mapper = teamin => teamMap(teamin)\nval mapperudf = udf(mapper)\n\n\n// Date Logic to adjust for games that finish on the day after ....\n// This is due to not having a great key to join my tables ...\nval datecrossregex = new Regex(\"^0[0-3]\")\nval dateadjust : ((String, String) => String) = (datein, tsin ) => {\n    val datetest =  datecrossregex.findFirstIn(tsin)\n    val dateary = datein.split(\"-\")\n    val rv = datetest match {\n      case Some(s) => { \n         val day = \"%02d\".format(dateary(2).toInt -1)\n         val newdate = dateary(0) + \"-\" + dateary(1) + \"-\" + day  \n         newdate\n      }\n      case None => datein\n    }\n    rv.asInstanceOf[String]\n}\nval dateadjustudf = udf(dateadjust)\n\n// UDFs to create some extra features ... this one is for an experiemental combination of Time left and Score difference.  \n// Made this via intuition.  This can be extended to add other custom features\n//val crossOverTime = 8\n//val exponentScaler = 0.5\nval scoredivtimeXform: ((Double,Double,Double,Double) => Double) = (sd:Double, tl:Double, co:Double, exp:Double) => {\n    val scaler = 1 / Math.pow( (tl / co) + 0.01 , exp)\n    sd * scaler\n}\nval scoredivtimeUdf = udf(scoredivtimeXform)\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###Preproces the Real Time and Final Score Data .  Add some useful columns to the data set"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 37, "source": "///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Here I create some extra columns for later use.  \n\n\n// Remove Overtime games from this analysis\nrtscoresAndFinalDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%OT%\"))\n\n// Create short 3 character team names \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teama\", mapperudf(col(\"teamlonga\")))\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"teamb\", mapperudf(col(\"teamlongb\")))\n\n// Add a score differential Column \nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\n\n// Transform the Date.  This is for games that spanned multiple days and gave me a headache.  \n// Games adjusted to the day they started on.\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"date\",  dateadjustudf($\"dateOrig\",$\"ts\"))\n\n// Create a Key for me to use to join with odds data later.  Key = date.teama.teamb\nrtscoresAndFinalDF = rtscoresAndFinalDF.withColumn(\"key\", concat($\"date\",lit(\".\"),$\"teama\",lit(\".\"),$\"teamb\"))"}, {"metadata": {}, "cell_type": "markdown", "source": "###Separate The Real Time And Final Data From One Common Dataframe To Two Dataframes"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 38, "source": "///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Currently based on the way the data was sampled, both real time scores and final scores are written as seperate records to the same file. \n// I need to pull these apart, and then join the dataframes so that I have a real time score and features and know if the game was won or lost ....\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Create Final Score DF\n// Note a shortcut for repeating the dataframe within the filter is to use a $   df.filter(df(\"foo\").contains ... is equiv to df.filter($\"foo\".contains)\n\nvar finalscoresDF = rtscoresAndFinalDF.filter($\"time-string\".like(\"%FINAL%\"))\n\n// Rename some columns so that join later doesnt have name overlaps\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scorea\", \"fscorea\")\nfinalscoresDF = finalscoresDF.withColumnRenamed(\"scoreb\", \"fscoreb\")\n\n// Create final score difference\nfinalscoresDF = finalscoresDF.withColumn(\"fscorea-fscoreb\", $\"fscorea\" - $\"fscoreb\")\nfinalscoresDF = finalscoresDF.withColumn(\"fscoreb-fscorea\", $\"fscoreb\" - $\"fscorea\")\n\n// Add a Win/loss column Win = 1, Loss = 0\nfinalscoresDF = finalscoresDF.withColumn(\"away-win\", ($\"fscorea-fscoreb\" > 0).cast(\"double\"))\nfinalscoresDF = finalscoresDF.withColumn(\"home-win\", ($\"fscoreb-fscorea\" > 0).cast(\"double\"))\n\n///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Create Real time score DF and more wrangling\n\n// Remove Halftime records and these other cases as my datasource doesnt always change the quarter well\n// as this particular case isn't handled well... (for now)\nvar rtscoresDF = rtscoresAndFinalDF.filter(!$\"time-string\".like(\"%FINAL%\")).\n  filter(not($\"time-string\" === \"(HALFTIME)\")).\n  filter(not($\"time-string\" === \"(12:00 IN 1ST)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 2ND)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 3RD)\") ).\n  filter(not($\"time-string\" === \"(12:00 IN 4TH)\") ).\n  filter(not($\"time-string\" === \"(END OF 1ST)\") ).\n  filter(not($\"time-string\" === \"(END OF 2ND)\") ).\n  filter(not($\"time-string\" === \"(END OF 3RD)\") ).\n  filter(not($\"time-string\" === \"(END OF 4TH)\") )\n\n// Create real time score difference\nrtscoresDF = rtscoresDF.withColumn(\"scorea-scoreb\", $\"scorea\" - $\"scoreb\")\nrtscoresDF = rtscoresDF.withColumn(\"scoreb-scorea\", $\"scoreb\" - $\"scorea\")\n\n// Create a game PCT complete and PCT left indictor\nrtscoresDF = rtscoresDF.withColumn(\"pct-complete\", ((($\"timeleft\" * -1) + 48 )/48.0)*100)\nrtscoresDF = rtscoresDF.withColumn(\"pct-left\", lit(100) - $\"pct-complete\" )\n\n// Create a unique feature based on my custom UDF.  Idea here is that I have intuition that timeleft and score difference are a strong predictor when combined\nrtscoresDF = rtscoresDF.withColumn(\"cf1\", scoredivtimeUdf($\"scoreb-scorea\", $\"pct-left\", lit(25.0), lit(0.5)))\nrtscoresDF = rtscoresDF.withColumn(\"cf2\", scoredivtimeUdf($\"scoreb-scorea\", $\"pct-left\", lit(2.0), lit(1.3)))\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###Custom Feature Explanation\nAfter building my initial model, I noticed that the logistic model was adjusting the probabilities well at the end of the games. I had some examples where I had 0 time left in the game, and yet the logistic model was giving a 70% chance of victory for a team. I speculated this was due to the fact that my original features were not fitting the end of game very well. To fix this, I created a spreader custom feature that basically takes the score difference and amplifies it as the score nears the end of the game. This way this feature is very predictive at the end of games and can help adjust the probablities to be more certain at the end of games."}, {"metadata": {}, "cell_type": "markdown", "source": "### TODO - Insert Graphs ..."}, {"metadata": {}, "cell_type": "markdown", "source": "###Lets Take A Look Of What We Have For The Two Dataframes We Just Wrangled"}, {"outputs": [{"output_type": "stream", "text": "final scores data frame\n+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+---------------+--------+--------+\n|  dateOrig|      ts|  teamlonga|fscorea|   teamlongb|fscoreb|time-string|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|fscorea-fscoreb|fscoreb-fscorea|away-win|home-win|\n+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+---------------+--------+--------+\n|2016-04-05|21:22:09|New Orleans|     93|Philadelphia|    107|    (FINAL)|     0.0|400829041|  nor|  phi|          -14|2016-04-05|2016-04-05.nor.phi|            -14|             14|     0.0|     1.0|\n|2016-04-05|22:08:42|  Charlotte|     90|     Toronto|     96|    (FINAL)|     0.0|400829043|  cha|  tor|           -6|2016-04-05|2016-04-05.cha.tor|             -6|              6|     0.0|     1.0|\n|2016-04-05|22:25:25|    Chicago|     92|     Memphis|    108|    (FINAL)|     0.0|400829045|  chi|  mem|          -16|2016-04-05|2016-04-05.chi.mem|            -16|             16|     0.0|     1.0|\n|2016-04-05|22:28:58|    Phoenix|     90|     Atlanta|    103|    (FINAL)|     0.0|400829044|  pho|  atl|          -13|2016-04-05|2016-04-05.pho.atl|            -13|             13|     0.0|     1.0|\n|2016-04-05|22:30:29|  Cleveland|    109|   Milwaukee|     80|    (FINAL)|     0.0|400829046|  cle|  mil|           29|2016-04-05|2016-04-05.cle.mil|             29|            -29|     1.0|     0.0|\n+----------+--------+-----------+-------+------------+-------+-----------+--------+---------+-----+-----+-------------+----------+------------------+---------------+---------------+--------+--------+\nonly showing top 5 rows\n\nTotal Games = 116\nreal time scores data frame\n+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+-------------+------------+--------+---+---+\n|  dateOrig|      ts|    teamlonga|scorea|   teamlongb|scoreb| time-string|timeleft|   gameid|teama|teamb|scorea-scoreb|      date|               key|scoreb-scorea|pct-complete|pct-left|cf1|cf2|\n+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+-------------+------------+--------+---+---+\n|2016-04-05|15:06:16|      Phoenix|     0|     Atlanta|     0|(8:00 PM ET)|    48.0|400829044|  pho|  atl|            0|2016-04-05|2016-04-05.pho.atl|            0|         0.0|   100.0|0.0|0.0|\n|2016-04-05|15:06:16|      Chicago|     0|     Memphis|     0|(8:00 PM ET)|    48.0|400829045|  chi|  mem|            0|2016-04-05|2016-04-05.chi.mem|            0|         0.0|   100.0|0.0|0.0|\n|2016-04-05|15:06:16|    Cleveland|     0|   Milwaukee|     0|(8:00 PM ET)|    48.0|400829046|  cle|  mil|            0|2016-04-05|2016-04-05.cle.mil|            0|         0.0|   100.0|0.0|0.0|\n|2016-04-05|15:06:16|Oklahoma City|     0|      Denver|     0|(9:00 PM ET)|    48.0|400829047|  okc|  den|            0|2016-04-05|2016-04-05.okc.den|            0|         0.0|   100.0|0.0|0.0|\n|2016-04-05|15:06:16|  New Orleans|     0|Philadelphia|     0|(7:00 PM ET)|    48.0|400829041|  nor|  phi|            0|2016-04-05|2016-04-05.nor.phi|            0|         0.0|   100.0|0.0|0.0|\n+----------+--------+-------------+------+------------+------+------------+--------+---------+-----+-----+-------------+----------+------------------+-------------+------------+--------+---+---+\nonly showing top 5 rows\n\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 39, "source": "// Some Printouts .....\nprintln(\"final scores data frame\")\nfinalscoresDF.show(5)\nprintln(\"Total Games = \" + finalscoresDF.count())\nprintln(\"real time scores data frame\")\nrtscoresDF.show(5)\nprintln(\"Total Number of rt score records = \" + rtscoresDF.count())\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###Inspect Raw Odds Data"}, {"outputs": [{"output_type": "stream", "text": "swift://notebooks.spark/scores_nba.test.dat\nDaily Odds Info\n--2016-09-13 12:16:45--  https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/nbaodds_042516.xml\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 21937 (21K) [text/plain]\nSaving to: 'nbaodds_042516.xml'\n\n     0K .......... .......... .                               100% 51.3M=0s\n\n2016-09-13 12:16:45 (51.3 MB/s) - 'nbaodds_042516.xml' saved [21937/21937]\n\nHistorical Odds Info\n", "name": "stdout"}, {"output_type": "execute_result", "metadata": {}, "execution_count": 40, "data": {"text/plain": "0"}}, {"output_type": "stream", "text": "basketball_nba.040516.xml:        <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\nbasketball_nba.040516.xml:        <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\nbasketball_nba.040516.xml:        <title>Charlotte 4.0 O (200.5) 155.0 | Toronto -4.0 U (200.5) -175.0 (Apr 05, 2016 07:40 PM)</title>\nbasketball_nba.040516.xml:        <title>Phoenix 14.5 O (207.5) -110.0 | Atlanta -14.5 U (207.5) -110.0 (Apr 05, 2016 08:10 PM)</title>\nbasketball_nba.040516.xml:        <title>Chicago -3.0 O (201.5) -150.0 | Memphis 3.0 U (201.5) 130.0 (Apr 05, 2016 08:10 PM)</title>\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 40, "source": "println(nbafile)\nimport sys.process._\n\n// cd /resources/data/\n\"rm -rf  nbaodds_042516.xml\".!\n\"echo Daily Odds Info\".!\n\"wget https://raw.githubusercontent.com/dustinvanstee/nba-rt-prediction/master/nbaodds_042516.xml\".!\n\"grep title nbaodds_042516.xml\"\n\n\"echo Historical Odds Info\".!\n\"head -n 5 nbaodds_042516.xml\".!\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###How to Read the Raw Odds Data\nHow to interpret the odds data ...\n    \n    Example Golden State -12.5 O (207.0) -125.0 | Detroit 12.5 U (207.0) 145.0\n    The away team is listed first, and the home team is second\n    Here Golden State is a 12.5 pt favorite to win.  The over under is in parentheses (207) and is the 50/50 line between teams sum of scores\n    being above/below that line.  \n    Finally the -125 / +145 numbers are whats known at the moneyline odds. \n        A negative number means you need to bet 125$ to get a 100$ payout\n        A positive number means you need to bet 100$ to get a 145$ payout"}, {"metadata": {}, "cell_type": "markdown", "source": "###Load in Odds Data"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 48, "source": "///////////////////////////////////////////////////////////////////////////////////////////////////////////////\n// Here, the data is very raw, and needs to be pre-processed .  I will start by loading it as an RDD and perform a \n// lot of transformations.  Once I have it properly parsed, I will convert to a dataframe.\n// This is not beautiful, but gets the job done\n// Data format .....\n//       <title>New Orleans 2.5 O (207.0) 125.0 | Phila. -2.5 U (207.0) -145.0 (Apr 05, 2016 07:10 PM)</title>\n//       <title>Detroit 4.0 O (202.0) 160.0 | Miami -4.0 U (202.0) -190.0 (Apr 05, 2016 08:05 PM)</title>\n\n// Reading the data in as an RDD first.  There isn't a dataframe parser for this XML I have, so I will write a custom parser ....\nval oddsrdd = sc.textFile(oddsfile)\n\n// just grabbing the text within the < ... > tags.  I can do this, because the format is super simple and not nested\nval gameStringRdd = oddsrdd.map(x => x.substring(x.indexOf('>')+1,x.lastIndexOf('<')))\n\n// String to Double converter helper\ndef parseDouble(s: String) = try { Some(s.toDouble) } catch { case _ : Throwable => None }\n\n// This is where I do the heavy lifting of parsing my XML .. and then finally convert my RDD to a dataframe .....\n// just lots of string parsing and data type conversions\nval oddsDF = gameStringRdd.map(x => {\n  // find the period, and then find the space prior ot the period\n  // Philadelphia special case.  since later on I index on a decimal point, I am assuming its part of a number and not a team abbrev\n  // also removing commas with the cryptic filterNot section ...  \n   val x1 = x.replace(\"Phila.\", \"Philadelphia\").filterNot(\",\" contains _)\n   val ss1 = x1.substring(0,x1.indexOf('.'))\n   val teamlonga = ss1.substring(0,ss1.lastIndexOf(' '))\n   val teama = teamMap(teamlonga)\n   val ss2 = x1.substring(ss1.lastIndexOf(' ')+1,x1.length)\n   val teamaspread = parseDouble(ss2.substring(0,ss2.indexOf(' ')) )\n   val ss3 = ss2.substring(ss2.indexOf(' ')+1,ss2.length)\n   val overunder = parseDouble(ss3.substring(ss3.indexOf('(')+1,ss3.indexOf(')')))\n   val ss4 = ss3.substring(ss3.indexOf(')')+2,ss3.length)\n   val teamaml = parseDouble(ss4.substring(0,ss4.indexOf(' ')))\n   val ss5x = ss4.substring(ss4.indexOf('|')+2,ss4.length)\n   val ss5 = ss5x.substring(0,ss5x.indexOf('.'))\n   \n   val teamlongb = ss5.substring(0,ss5.lastIndexOf(' '))\n   val teamb = teamMap(teamlongb)\n   val ss6 = ss5x.substring(ss5.lastIndexOf(' ')+1,ss5x.length)\n\n   val teambml = parseDouble(ss6.substring(ss6.indexOf(')')+2,ss6.lastIndexOf('(')-1))\n   val ss7 = ss6.substring(ss6.lastIndexOf('(')+1,ss6.length)\n   val dateInfo = ss7.split(' ')\n   val dateStr = dateInfo(2) + \"-\" + monthMap(dateInfo(0)) + \"-\" + dateInfo(1)\n   // This will become my join key for the other data sets\n   val key = dateStr +\".\" + teama + \".\" + teamb\n  (key,teamlonga,teama,teamaspread,overunder,teamaml,teamlongb,teamb,teambml,dateStr)\n   \n  }\n  // Had to add the groupby and average below because I was getting the game odds over multiple days, and it was \n  // adding noise to the analysis\n  // Rename some columns\n  // Create a few new columns for later analysis\n\n).toDF(\"key\",\"teamlonga\",\"teama\",\"teamaspread\",\"overunder\",\"teamaml\",\"teamlongb\",\"teamb\",\"teambml\",\"dateStr\").\n    groupBy(\"key\",\"teamlonga\",\"teama\",\"teamlongb\",\"teamb\",\"dateStr\").\n    agg(avg(\"teamaspread\"),avg(\"overunder\"),avg(\"teamaml\"),avg(\"teambml\")).\n    withColumnRenamed(\"avg(teamaspread)\",\"teamaspread\").\n    withColumnRenamed(\"avg(overunder)\",\"overunder\").\n    withColumnRenamed(\"avg(teamaml)\",\"teamaml\").\n    withColumnRenamed(\"avg(teambml)\",\"teambml\").\n    withColumn(\"teambspread\", $\"teamaspread\" * -1).\n    withColumn(\"teama_vegas_fscore\", $\"overunder\" / 2.0 - $\"teamaspread\"/2.0).\n    withColumn(\"teamb_vegas_fscore\", $\"overunder\" / 2.0 + $\"teamaspread\"/2.0)\n\noddsDF.registerTempTable(\"oddsDF\")\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###Inspect Some Of The Odds Data"}, {"outputs": [{"output_type": "stream", "text": "+------------------+-------------+-----+-----------+-----+----------+-----------+---------+-------+-------+-----------+------------------+------------------+\n|               key|    teamlonga|teama|  teamlongb|teamb|   dateStr|teamaspread|overunder|teamaml|teambml|teambspread|teama_vegas_fscore|teamb_vegas_fscore|\n+------------------+-------------+-----+-----------+-----+----------+-----------+---------+-------+-------+-----------+------------------+------------------+\n|2016-04-12.mem.lac|      Memphis|  mem|LA Clippers|  lac|2016-04-12|        7.5|    197.0|  280.0| -340.0|       -7.5|             94.75|            102.25|\n|2016-04-06.det.orl|      Detroit|  det|    Orlando|  orl|2016-04-06|       1.25|   209.25|    7.5| -127.5|      -1.25|             104.0|            105.25|\n|2016-04-13.uta.lal|         Utah|  uta|  LA Lakers|  lal|2016-04-13|       -5.0|    193.5| -210.0|  175.0|        5.0|             99.25|             94.25|\n|2016-04-09.okc.sac|Oklahoma City|  okc| Sacramento|  sac|2016-04-09|       -7.0|    224.5| -300.0|  250.0|        7.0|            115.75|            108.75|\n|2016-04-07.tor.atl|      Toronto|  tor|    Atlanta|  atl|2016-04-07|        7.0|    200.0|  250.0| -300.0|       -7.0|              96.5|             103.5|\n+------------------+-------------+-----+-----------+-----+----------+-----------+---------+-------+-------+-----------+------------------+------------------+\nonly showing top 5 rows\n\nTotal Home Teams      = 30\nTotal Away Teams      = 30\nTotal Games Collected = 111\n ", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 50, "source": "oddsDF.show(5)\nprintf(\"Total Home Teams      = %2d\\n\",oddsDF.select(\"teama\").distinct.sort(\"teama\").count())\nprintf(\"Total Away Teams      = %2d\\n\",oddsDF.select(\"teamb\").distinct.sort(\"teamb\").count())\nprintf(\"Total Games Collected = %d\\n \",oddsDF.count())\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Insert Graphs Here"}, {"metadata": {}, "cell_type": "markdown", "source": "###Join The Odds And Final Score Data Sets"}, {"outputs": [{"output_type": "stream", "text": "gameDF\n+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\n|   dateStr|teamaspread|overunder|teamaml|teambml|teambspread|teama_vegas_fscore|teamb_vegas_fscore|               key|fscorea|fscoreb|fscorea-fscoreb|fscoreb-fscorea|away-win|home-win|\n+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\n|2016-04-12|        7.5|    197.0|  280.0| -340.0|       -7.5|             94.75|            102.25|2016-04-12.mem.lac|     84|    110|            -26|             26|     0.0|     1.0|\n|2016-04-06|       1.25|   209.25|    7.5| -127.5|      -1.25|             104.0|            105.25|2016-04-06.det.orl|    108|    104|              4|             -4|     1.0|     0.0|\n|2016-04-13|       -5.0|    193.5| -210.0|  175.0|        5.0|             99.25|             94.25|2016-04-13.uta.lal|     96|    101|             -5|              5|     0.0|     1.0|\n+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\nonly showing top 3 rows\n\nTotal Joined Games Collected = 103\n ", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 52, "source": "// Here is where we join the Odds/Realtime scores/ Final Scores into one wholistic data set as input for Logistic Machine Learning\n\n// Create a smaller Final Score Dataframe.  Just keep the key, final score a and b, the win/loss indicator\nval finalslicedscoresDF = finalscoresDF.select($\"key\",$\"fscorea\",$\"fscoreb\",$\"fscorea-fscoreb\",$\"fscoreb-fscorea\",$\"away-win\",$\"home-win\")\n\n// First Join the 2 smallest data frames ... odd and final.\nval gameDF = oddsDF.join(finalslicedscoresDF, oddsDF(\"key\") === finalscoresDF(\"key\")).drop(oddsDF(\"key\")).\ndrop(\"teamlonga\").\ndrop(\"teamlongb\").\ndrop(\"teama\").\ndrop(\"teamb\")\n\n// Print Out the Game Dataframe ... notice we have the odds data merged with the win loss data ....\nprintln(\"gameDF\")\ngameDF.show(3)\nprintf(\"Total Joined Games Collected = %d\\n \",gameDF.count())\ngameDF.registerTempTable(\"gameDF\")\n"}, {"metadata": {}, "cell_type": "markdown", "source": "### Insert graphs here"}, {"metadata": {}, "cell_type": "markdown", "source": "###Join The Game Dataframe With The Real Time Score Dataframe"}, {"outputs": [{"output_type": "stream", "text": "lrDF : Logistic Regression Data Frame\n+----------+--------+---------+------+-----------+------+--------------+-------------+---------+-----+-----+-------------+----------+-------------+-------------------+-----------------+-------------------+--------------------+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\n|  dateOrig|      ts|teamlonga|scorea|  teamlongb|scoreb|   time-string|     timeleft|   gameid|teama|teamb|scorea-scoreb|      date|scoreb-scorea|       pct-complete|         pct-left|                cf1|                 cf2|   dateStr|teamaspread|overunder|teamaml|teambml|teambspread|teama_vegas_fscore|teamb_vegas_fscore|               key|fscorea|fscoreb|fscorea-fscoreb|fscoreb-fscorea|away-win|home-win|\n+----------+--------+---------+------+-----------+------+--------------+-------------+---------+-----+-----+-------------+----------+-------------+-------------------+-----------------+-------------------+--------------------+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\n|2016-04-12|12:26:12|  Memphis|     0|LA Clippers|     0| (10:30 PM ET)|         48.0|400829103|  mem|  lac|            0|2016-04-12|            0|                0.0|            100.0|                0.0|                 0.0|2016-04-12|        7.5|    197.0|  280.0| -340.0|       -7.5|             94.75|            102.25|2016-04-12.mem.lac|     84|    110|            -26|             26|     0.0|     1.0|\n|2016-04-12|22:46:30|  Memphis|     0|LA Clippers|     0|(11:53 IN 1ST)|47.8833333333|400829103|  mem|  lac|            0|2016-04-12|            0|0.24305555562499728|99.75694444437501|                0.0|                 0.0|2016-04-12|        7.5|    197.0|  280.0| -340.0|       -7.5|             94.75|            102.25|2016-04-12.mem.lac|     84|    110|            -26|             26|     0.0|     1.0|\n|2016-04-12|22:47:01|  Memphis|     0|LA Clippers|     1|(11:53 IN 1ST)|47.8833333333|400829103|  mem|  lac|           -1|2016-04-12|            1|0.24305555562499728|99.75694444437501|0.49998263979323465|0.006202970765147405|2016-04-12|        7.5|    197.0|  280.0| -340.0|       -7.5|             94.75|            102.25|2016-04-12.mem.lac|     84|    110|            -26|             26|     0.0|     1.0|\n+----------+--------+---------+------+-----------+------+--------------+-------------+---------+-----+-----+-------------+----------+-------------+-------------------+-----------------+-------------------+--------------------+----------+-----------+---------+-------+-------+-----------+------------------+------------------+------------------+-------+-------+---------------+---------------+--------+--------+\nonly showing top 3 rows\n\nTotal Data Points in DataSet = 13412\n ", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 54, "source": "// This is the bigger merge.  Merging the odds/final score data with the real time indicators ...\n\nvar lrDF = rtscoresDF.join(gameDF, rtscoresDF(\"key\") === gameDF(\"key\")).drop(gameDF(\"key\"))\n\nprintln(\"lrDF : Logistic Regression Data Frame\")\nlrDF.show(3)\nprintf(\"Total Data Points in DataSet = %d\\n \",lrDF.count())\nlrDF.registerTempTable(\"lrDF\")"}, {"metadata": {}, "cell_type": "markdown", "source": "###Add a Few More Features"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 56, "source": "///////////////////////////////////////////////\n// Add an overunder/spread adjusted projection \n// as points are scored during the game\n// I found this is a strong indicator\nlrDF = lrDF.withColumn(\"teama_adj_fscore\",(( $\"pct-complete\" * -1)/100 + 1) *  $\"teama_vegas_fscore\" + $\"scorea\").\n            withColumn(\"teamb_adj_fscore\",(( $\"pct-complete\" * -1)/100 + 1) *  $\"teamb_vegas_fscore\" + $\"scoreb\").\n            withColumn(\"pfscoreb-pfscorea\", $\"teamb_adj_fscore\" - $\"teama_adj_fscore\" )\n                   "}, {"metadata": {}, "cell_type": "markdown", "source": "###Filter Out Some Data Due To Data Quality"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 60, "source": "// There is an issue with the data I had captured.  When a quarter transitions from 1st->2nd (etc,etc), sometime the timestring doesn't get updated properly.  Since I used the timestring to calculate the timeleft in the game, I would get some rogue data points.  \n// Example, after 1 min in a game, something the two teams would have scores in the 20's, because it was really at 11 mins in the second quarter.  \n// My solution was to use the final score sum, and then just scale that down to the time left in the game.  I would then compare to the sum of scores i had, and if it was significantly higher, I would remove them.  I did this by visual inspection ... \n// dfa = departure_from_avg\nlrDF = lrDF.withColumn(\"dfa\",(($\"fscorea\" + $\"fscoreb\")/48) * ($\"timeleft\" * -1 + 48) -( $\"scorea\" + $\"scoreb\")).orderBy(asc(\"dfa\")).filter($\"dfa\" > -30)\n"}, {"metadata": {}, "cell_type": "markdown", "source": "###Lets Look At Some Stats From Logistic Regression Dataframe"}, {"outputs": [{"output_type": "stream", "text": "+-------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n|summary|            scorea|            scoreb|          timeleft|             gameid|     scorea-scoreb|    scoreb-scorea|      pct-complete|          pct-left|               cf1|                cf2|       teamaspread|        overunder|           teamaml|            teambml|        teambspread|teama_vegas_fscore|teamb_vegas_fscore|           fscorea|           fscoreb|   fscorea-fscoreb|   fscoreb-fscorea|          away-win|          home-win|  teama_adj_fscore|  teamb_adj_fscore| pfscoreb-pfscorea|                dfa|\n+-------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n|  count|             13217|             13217|             13217|              13217|             13217|            13217|             13217|             13217|             13217|              13217|             13217|            13217|             13217|              13217|              13217|             13217|             13217|             13217|             13217|             13217|             13217|             13217|             13217|             13217|             13217|             13217|              13217|\n|   mean|52.849360671861994| 56.14269501399713|22.404877556681914|4.008397326615722E8|-3.293334342135129|3.293334342135129| 53.32317175691261|46.676828243087385| 4.798907343641492| 31.508478303098755|3.5182025674005715| 204.837141560112|26.409674409220457|-122.39615646515847|-3.5182025674005715| 100.6594694963557|104.17767206375626| 99.71256714836953|105.30226223802678|-5.589695089657259| 5.589695089657259|0.3529545282590603|0.6470454717409397| 99.85102228422996|104.78852466831235| 4.937502384082339| 0.4490646515856353|\n| stddev| 30.31373365865063|32.071352423580656| 14.02475280021302|  19211.61424790004| 11.78790619030189|11.78790619030189|29.218235000443787|29.218235000443787|28.667061033878944|  850.6837829216923| 7.670173100438858|8.906410354070108|252.40409518334664|  257.2292748740361|  7.670173100438858| 5.386032235608904| 6.329968969108246|12.152291813579295| 11.93188225052894|14.790081576860304|14.790081576860304|0.4779068001505168|0.4779068001505168|10.091200959355476|10.599071878291188|13.470132192226565| 7.0282429764294925|\n|    min|                 0|                 0|               0.0|          400829041|               -44|              -33|               0.0|               0.0|            -290.0|-11545.107946051425|             -13.0|            180.5|-553.3333333333334|             -750.0|              -19.0|            84.375| 84.91666666666667|                68|                80|               -38|               -29|               0.0|               0.0| 66.41608796296988|              75.5|-34.91545138889478|-26.786458333493727|\n|    max|               131|               144|              48.0|          400874419|                33|               44|             100.0|             100.0|             380.0| 15128.072481032901|              19.0|           225.25| 541.6666666666666|              410.0|               13.0|            115.75|           119.125|               131|               144|                29|                38|               1.0|               1.0| 135.3515625000781|146.36163194445217| 47.36250000000001| 21.311458333481255|\n+-------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+------------------+------------------+-------------------+------------------+-----------------+------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+\n\n", "name": "stdout"}], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 61, "source": " lrDF.describe().show()"}, {"metadata": {}, "cell_type": "markdown", "source": "### Visualize Time Series Data - TBD"}, {"metadata": {}, "cell_type": "markdown", "source": "### Save Out Dataframe For Further Analysis with Logistic and Linear Regression Notebooks"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 64, "source": "// Wanted to save out the dataset at this point as I will branch into seperate work efforts for a Logistic/Linear model building\n// drop some columns as we move on to next step !!  \n\nlrDF.drop(\"dateOrig\").\n    drop(\"ts\").\n    drop(\"teamlonga\").\n    drop(\"teamlongb\").\n    drop(\"time-string\").\n    drop(\"gameid\").\n    drop(\"date\").\n    drop(\"dateStr\").\n    drop(\"teamaml\").\n    drop(\"teambml\").\n    drop(\"dfa\").\n    write.format(\"com.databricks.spark.csv\").\n    option(\"header\", \"true\"). // Use first zline of all files as header\n    option(\"inferSchema\", \"false\"). // Automatically infer data types)\n    option(\"nullValue\", \"empty\").\n    option(\"dateFormat\", \"yyyy-MM-dd\").\n    option(\"mode\",\"DROPMALFORMED\").\n    save(\"nba-datawrangle-lrDF.csv\")\n"}, {"outputs": [], "metadata": {"collapsed": false}, "cell_type": "code", "execution_count": 67, "source": "// TODO ## cat a file in swift ?? Prob not possible\nval oddsfile = \"swift://notebooks.spark/nba-datawrangle-lrDF.csv\"\n\nlrDF.drop(\"dateOrig\").\n    drop(\"ts\").\n    drop(\"teamlonga\").\n    drop(\"teamlongb\").\n    drop(\"time-string\").\n    drop(\"gameid\").\n    drop(\"date\").\n    drop(\"dateStr\").\n    drop(\"teamaml\").\n    drop(\"teambml\").\n    drop(\"dfa\").\n    write.format(\"com.databricks.spark.csv\").\n    option(\"header\", \"true\"). // Use first zline of all files as header\n    option(\"inferSchema\", \"false\"). // Automatically infer data types)\n    option(\"nullValue\", \"empty\").\n    option(\"dateFormat\", \"yyyy-MM-dd\").\n    option(\"mode\",\"DROPMALFORMED\").\n    save(oddsfile)\n\n"}, {"outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "execution_count": null, "source": ""}], "nbformat_minor": 0}